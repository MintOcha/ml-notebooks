{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinkercademy/ml-notebooks/blob/main/Generative AI/05_GAN_Example_Program_Digit_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0e7c912b",
      "metadata": {
        "id": "0e7c912b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3q5QvOSQAUNL",
      "metadata": {
        "id": "3q5QvOSQAUNL"
      },
      "source": [
        "Sets the seed of any random number generating function, so that the generated numbers are reproducible.\n",
        "\n",
        "e.g. torch.rand(2) will consistently give you tensor([0.7156, 0.9140]) with torch.manual_seed(111)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "25a8b17e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25a8b17e",
        "outputId": "3e762b16-165e-44dd-d1b8-f2293f1abb7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f1efc430f50>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(121)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H_D2VBjjAZt8",
      "metadata": {
        "id": "H_D2VBjjAZt8"
      },
      "source": [
        "Uses GPU if it's available, and CPU otherwise. (GPU would be faster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a57e36f4",
      "metadata": {
        "id": "a57e36f4"
      },
      "outputs": [],
      "source": [
        "device = \"\"\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QIvyeIDWAc7l",
      "metadata": {
        "id": "QIvyeIDWAc7l"
      },
      "source": [
        "Transformation required to convert the dataset to a pytorch Tensor, and to set the range of coefficients to be from -1 to 1.\n",
        "\n",
        "The output of Normalize(mean, std) = (Tensor input - mean) / std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8c36b1a6",
      "metadata": {
        "id": "8c36b1a6"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QlTiZdHa4u-Y",
      "metadata": {
        "id": "QlTiZdHa4u-Y"
      },
      "source": [
        "Downloads the MNIST dataset from pytorch, after transforming it into the required format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9675f2e0",
      "metadata": {
        "id": "9675f2e0"
      },
      "outputs": [],
      "source": [
        "train_set = torchvision.datasets.MNIST(\n",
        "    root=\".\", train=True, download=True, transform=transform\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y0c8mha044xe",
      "metadata": {
        "id": "Y0c8mha044xe"
      },
      "source": [
        "Loads the dataset into a pytorch dataloader to obtain shuffled batches of 32 samples. The batch size is the number of samples we pass into the training loop at each iteration, while shuffle is useful to have real and generated data in each batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "abd378c8",
      "metadata": {
        "id": "abd378c8"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set, batch_size=batch_size, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6be39bde",
      "metadata": {
        "id": "6be39bde"
      },
      "outputs": [],
      "source": [
        "real_samples, mnist_labels = next(iter(train_loader))\n",
        "for i in range(16):\n",
        "    ax = plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(real_samples[i].reshape(28, 28), cmap=\"gray_r\")\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M6IxQhP4Ambd",
      "metadata": {
        "id": "M6IxQhP4Ambd"
      },
      "source": [
        "Our Discriminator model which contains the neural network in self.model.\n",
        "\n",
        "The forward method of the Discriminator works the same way \\_\\_call\\_\\_ does for a regular python class (i.e. runs when Discriminator(arg) is called)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "94db4290",
      "metadata": {
        "id": "94db4290"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(784, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), 784)\n",
        "        output = self.model(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3313e4f3",
      "metadata": {
        "id": "3313e4f3"
      },
      "outputs": [],
      "source": [
        "discriminator = Discriminator().to(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CrOzxak6Aqjq",
      "metadata": {
        "id": "CrOzxak6Aqjq"
      },
      "source": [
        "Our Generator model which contains the neural network in self.model.\n",
        "\n",
        "The forward method of the Generator works the same way \\_\\_call\\_\\_ does for a regular python class (i.e. runs when Generator(arg) is called)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f86f1cf8",
      "metadata": {
        "id": "f86f1cf8"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(100, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 784),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        output = output.view(x.size(0), 1, 28, 28)\n",
        "        return output\n",
        "\n",
        "generator = Generator().to(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BiP115qm6bQB",
      "metadata": {
        "id": "BiP115qm6bQB"
      },
      "source": [
        "_lr_ refers to the learning rate of the model.\n",
        "\n",
        "_num\\_epochs_ refers to how many repetitions of training will be done using the whole training set.\n",
        "\n",
        "_loss\\_function_ refers to the loss function (i.e. binary cross-entropy) used by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7193d9ef",
      "metadata": {
        "id": "7193d9ef"
      },
      "outputs": [],
      "source": [
        "lr = 0.0001\n",
        "num_epochs = 50\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "\n",
        "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
        "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S64RqjAA78ic",
      "metadata": {
        "id": "S64RqjAA78ic"
      },
      "source": [
        "The standard way of organising data in pytorch is to have each line of the tensor to represent one sample from the batch.\n",
        "\n",
        "torch.ones(_size_) returns a tensor filled with scalar value 1, with shape of argument _size_. This is to represent they are real images.\n",
        "\n",
        "latent\\_space\\_samples represents a tensor filled with random numbers, of size (batch_size, 100). 100 matches the input size of the Generator input, it will be used as a random input to generate.\n",
        "\n",
        "torch.zeros(_size_) returns a tensor filled with scalar value 0, with shape of argument _size_. This is to represent they are generated images.\n",
        "\n",
        "torch.cat((a, b)) concatenates tensors a and b.\n",
        "\n",
        "discriminator.zero_grad() is required to clear the gradients at each training step.\n",
        "\n",
        "loss_discriminator.backward() calculates gradients to update the weights. optimizer_discriminator.step() actually updates the discriminator weights. Same for generator. This is backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44932f3d",
      "metadata": {
        "id": "44932f3d"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for n, (real_samples, mnist_labels) in enumerate(tqdm(train_loader)):\n",
        "        # Data for training the discriminator\n",
        "        real_samples = real_samples.to(device=device)\n",
        "        real_samples_labels = torch.ones((batch_size, 1)).to(\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Generate random noise (tensors of random values) as input for the Generator\n",
        "        latent_space_samples = torch.randn((batch_size, 100)).to(\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # We get generated samples to train the discriminator with\n",
        "        generated_samples = generator(latent_space_samples)\n",
        "        generated_samples_labels = torch.zeros((batch_size, 1)).to(\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # We concatenate the real and generated samples into one group\n",
        "        all_samples = torch.cat((real_samples, generated_samples))\n",
        "        all_samples_labels = torch.cat(\n",
        "            (real_samples_labels, generated_samples_labels)\n",
        "        )\n",
        "\n",
        "        # Training the discriminator\n",
        "        discriminator.zero_grad()\n",
        "        output_discriminator = discriminator(all_samples)\n",
        "        loss_discriminator = loss_function(\n",
        "            output_discriminator, all_samples_labels\n",
        "        )\n",
        "        loss_discriminator.backward()\n",
        "        optimizer_discriminator.step()\n",
        "\n",
        "        # Data for training the generator, we generate random noise as input again\n",
        "        latent_space_samples = torch.randn((batch_size, 100)).to(\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Training the generator\n",
        "        generator.zero_grad()\n",
        "        generated_samples = generator(latent_space_samples)\n",
        "        output_discriminator_generated = discriminator(generated_samples)\n",
        "        loss_generator = loss_function(\n",
        "            output_discriminator_generated, real_samples_labels\n",
        "        )\n",
        "        loss_generator.backward()\n",
        "        optimizer_generator.step()\n",
        "\n",
        "        # Show loss\n",
        "        if n == batch_size - 1:\n",
        "            print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n",
        "            print(f\"Epoch: {epoch} Loss G.: {loss_generator}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f188030d",
      "metadata": {
        "id": "f188030d"
      },
      "outputs": [],
      "source": [
        "# Training is over, time to generating random noise as input for Generator to produce the digits!\n",
        "latent_space_samples = torch.randn(batch_size, 100).to(device=device)\n",
        "generated_samples = generator(latent_space_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9dd4b71",
      "metadata": {
        "id": "c9dd4b71"
      },
      "outputs": [],
      "source": [
        "generated_samples = generated_samples.cpu().detach()\n",
        "for i in range(16):\n",
        "    ax = plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(generated_samples[i].reshape(28, 28), cmap=\"gray_r\")\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
